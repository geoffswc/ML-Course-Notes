# Note

This repository is for an ML workshop held in 2018. Many of the explanations and visuals may still be useful, and the code will still run. However, this code doesn't use more recent features in scikit-learn that simply the data pipeline. You may want to take a look at https://github.com/geoffswc/Scikit-Learn-Workshop for a more up-to-date introduction. 

# ML-Course-Notes

Notes for ML course at UCSF Library

## Visuals

### Decision Tree

<img src="supervised_learning_visuals/decision_tree.png" alt="hi" class="inline"/>

### Random Forest

<img src="supervised_learning_visuals/random_forest.png" alt="hi" class="inline"/>

### Logistic Regression

<img src="supervised_learning_visuals/logistic_regression.png" alt="hi" class="inline"/>

### Support Vector Machine

<img src="supervised_learning_visuals/support_vector_machine.png" alt="hi" class="inline"/>

### Neural Net

<img src="supervised_learning_visuals/neural_network.png" alt="hi" class="inline"/>

### Naive Bayes

<img src="supervised_learning_visuals/naive_bayes.png" alt="hi" class="inline"/>

## Exercises

Swap out the Random Forest for a different ML algorithm. At their defaults, how do they perform?

See if you can get better performance from the Random Forest by changing some of the parameters. What does n_estimators do?

Take a look at the parameters for a vectorizer. What does ngram_range do? Can you view the output of the ngram range?

Try programming a rules based (as opposed to machine learning) approach. Can you beat these algorithms?
